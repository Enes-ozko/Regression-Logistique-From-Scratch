# R√©gression Logistique "From Scratch" (NumPy)

Projet r√©alis√© en 2√®me ann√©e d'√©cole d'ing√©nieur √† l'ENSISA (Informatique & R√©seaux).
**Objectif :** Comprendre les fondements math√©matiques des r√©seaux de neurones en impl√©mentant un classifieur d'images multi-classes sans utiliser de frameworks de haut niveau (comme `sklearn` pour la logique du mod√®le).

## üìÇ Structure du Projet
* `data1.py` : Impl√©mentation sur le dataset **Digits** (images 8x8 pixels). Inclut la boucle d'entra√Ænement et la visualisation.
* `data2.py` : Impl√©mentation sur le dataset **MNIST** (images 28x28 pixels). Ajoute la **R√©gularisation L2** et une comparaison avec k-NN.
* `docs/` : Contient le rapport complet du projet (PDF).

## üõ†Ô∏è D√©tails d'Impl√©mentation
Le mod√®le est construit en **Python** pur avec **NumPy** pour la vectorisation.

### 1. Fondations Math√©matiques
Nous avons cod√© manuellement les composants suivants :
* **Mod√®le :** Score lin√©aire $Z = XW + b$
* **Activation :** Fonction Softmax pour la distribution de probabilit√©.
    $$\hat{y} = \text{softmax}(Z)$$
* **Fonction de Co√ªt :** Entropie Crois√©e (Cross-Entropy) avec r√©gularisation L2 optionnelle.
    $$J(W) = -\frac{1}{m} \sum y \log(\hat{y}) + \frac{\lambda}{2m} \|W\|^2$$
* **Optimisation :** Descente de Gradient par Batch (Batch Gradient Descent).

### 2. Fonctionnalit√©s Cl√©s
* **Op√©rations vectoris√©es :** Utilisation du calcul matriciel pour remplacer les boucles Python et optimiser les performances.
* **Hyperparam√®tres :** Ajustement du taux d'apprentissage ($\alpha$) et du nombre d'√©poques.
* **R√©gularisation :** Ridge (L2) impl√©ment√©e pour r√©duire le sur-apprentissage sur les donn√©es complexes (MNIST).

## üìä R√©sultats
Nous avons compar√© notre impl√©mentation "maison" avec `LogisticRegression` et `KNeighborsClassifier` de Scikit-Learn.

| Dataset | Notre Mod√®le (NumPy) | Baseline Sklearn | k-NN (k=3) |
| :--- | :--- | :--- | :--- |
| **Digits (8x8)** | **97.50%** | 97.22% | N/A |
| **MNIST (28x28)** | **89.90%** | 88.15% | 90.80% |